print(sum)
}
iphones.summary(iphones.no.X)
lapply(grouped, iphones.summary)
normalized = (iphones.no.X$NS-min(iphones.no.X$NS))/(max(iphones.no.X$NS)-min(iphones.no.X$NS))
grouped <- split(normalized, f = normalized$weekgroup)
grouped
lapply(grouped, normalize)
normalize <- function(iphones) {
return((iphones$NS - min(iphones$NS))/(max(iphones$NS) - min(iphones$NS)))
}
lapply(grouped, normalize)
grouped$NS <- lapply(grouped, normalize)
lapply(grouped, iphones.summary)
grouped
normalize <- function(iphones) {
return((iphones$NS - min(iphones$NS))/(max(iphones$NS) - min(iphones$NS)))
}
iphones.no.X$NS <- lapply(iphones.no.X, normalize)
grouped <- split(iphones.no.X, f = iphones.no.X$weekgroup)
lapply(grouped, iphones.summary)
lookupWeek <- (rep(1:round(52 / 4), each = 4) + 6) %% 13
names(lookupWeek) <- sprintf("%02d", 1:52)
weekgroup <- function(NS) {
return(unname(lookupWeek[substr(NS, 4, 5)]))
}
iphones.no.X <-
cbind(iphones.no.X, weekgroup = weekgroup(iphones.no.X$PC))
iphones.summary <- function(iphones) {
sum <- data.frame(
iphones.mean = mean(iphones$NS),
iphones.median = median(iphones$NS),
iphones.sd = sd(iphones$NS),
iphones.var = var(iphones$NS)
)
print(sum)
}
iphones.summary(iphones.no.X)
grouped <- split(iphones.no.X, f = iphones.no.X$weekgroup)
group.estimations <-
lapply(grouped, estimate)
print(ggplot(iphones.no.X) + geom_point(aes(x = weekgroup, y = NS)))
# On choisit alpha = 0.01.
# p-value < alpha --> on rejette H0
# H0 est que les observations suivent une distribution uniforme de leur valeur minimale à leur valeur maximale.
# test de Kolmogorov-Smirnov
ks.test(iphones.no.X$NS, "punif", min(iphones.no.X$NS), max(iphones.no.X$NS))
# chi-squared test
chisq.test(iphones.no.X$NS)
plot(ecdf(iphones.no.X$NS))
curve(punif(x, min(iphones.no.X$NS), max(iphones.no.X$NS)), add = T, col = "red")
ks <- function(iphones) {
return(ks.test(iphones$NS, "punif", min(iphones$NS), max(iphones$NS)))
}
chisq <- function(iphones) {
return(chisq.test(iphones$NS))
}
lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
group.chisq.test <- lapply(grouped, chisq)
normalize <- function(iphones) {
return((iphones$NS - min(iphones$NS))/(max(iphones$NS) - min(iphones$NS)))
}
iphones.no.X$NS <- lapply(iphones.no.X, normalize)
grouped <- split(iphones.no.X, f = iphones.no.X$weekgroup)
iphones.no.X[NS] <- lapply(iphones.no.X, normalize)
iphones.no.X[[NS]] <- lapply(iphones.no.X, normalize)
names(lookupWeek) <- sprintf("%02d", 1:52)
weekgroup <- function(NS) {
return(unname(lookupWeek[substr(NS, 4, 5)]))
}
iphones.no.X <-
cbind(iphones.no.X, weekgroup = weekgroup(iphones.no.X$PC))
iphones.summary <- function(iphones) {
sum <- data.frame(
iphones.mean = mean(iphones$NS),
iphones.median = median(iphones$NS),
iphones.sd = sd(iphones$NS),
iphones.var = var(iphones$NS)
)
print(sum)
}
iphones.summary(iphones.no.X)
grouped <- split(iphones.no.X, f = iphones.no.X$weekgroup)
group.estimations <-
lapply(grouped, estimate)
print(ggplot(iphones.no.X) + geom_point(aes(x = weekgroup, y = NS)))
# On choisit alpha = 0.01.
# p-value < alpha --> on rejette H0
# H0 est que les observations suivent une distribution uniforme de leur valeur minimale à leur valeur maximale.
# test de Kolmogorov-Smirnov
ks.test(iphones.no.X$NS, "punif", min(iphones.no.X$NS), max(iphones.no.X$NS))
# chi-squared test
chisq.test(iphones.no.X$NS)
plot(ecdf(iphones.no.X$NS))
curve(punif(x, min(iphones.no.X$NS), max(iphones.no.X$NS)), add = T, col = "red")
ks <- function(iphones) {
return(ks.test(iphones$NS, "punif", min(iphones$NS), max(iphones$NS)))
}
chisq <- function(iphones) {
return(chisq.test(iphones$NS))
}
lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
group.ks.test
group.estimations
str(group.estimations)
estimate(iphones.no.X)
estimate(iphones.no.X[1:5])
iphones.no.X[1:5]
iphones.no.X[1]
iphones.no.X[1,]
iphones.no.X[1:3,]
iphones.no.X[30:,]
iphones.no.X[30:139,]
plot(ecdf(iphones.no.X[30:139,]))
plot(ecdf(iphones.no.X[30:139,]$NS))
curve(punif(x, min(iphones.no.X[30:139,]$NS), max(iphones.no.X[30:139,]$NS)), add = T, col = "red")
plot(ecdf(iphones.no.X[60:139,]$NS))
plot(ecdf(iphones.no.X[iphones.no.X$NS > 1408740]$NS))
plot(ecdf(iphones.no.X[60:139,]$NS))
plot(ecdf(iphones.no.X[80:139,]$NS))
plot(ecdf(iphones.no.X[70:139,]$NS))
plot(ecdf(iphones.no.X[60:139,]$NS))
curve(punif(x, min(iphones.no.X[60:139,]$NS), max(iphones.no.X[60:139,]$NS)), add = T, col = "red")
estimate(iphones.no.X[60:139,])
estimate(iphones.no.X)
ks(iphones.no.X[60:139,])
ks(iphones.no.X[50:139,])
ks(iphones.no.X[55:139,])
estimate(iphones.no.X[55:139,])
estimate(iphones.no.X[60:139,])
ks(iphones.no.X[56:139,])
ks(iphones.no.X[57:139,])
ks(iphones.no.X[60:139,])
estimate(iphones.no.X[60:139,])
plot(ecdf(iphones.no.X[60:139,]$NS))
curve(punif(x, min(iphones.no.X[60:139,]$NS), max(iphones.no.X[60:139,]$NS)), add = T, col = "red")
ks(iphones.no.X[60:139,])
estimate(iphones.no.X[60:139,])
ks(iphones.no.X[53:139,])
ks(iphones.no.X[52:139,])
estimate(iphones.no.X[52:139,])
estimate(iphones.no.X)
knitr::opts_chunk$set(echo = TRUE)
# Affichages graphiques
plot_estimations <- function(estimations) {
for (name in names(estimations)) {
print(
qplot(
estimations[[name]],
main = paste("Distribution de l'estimateur", name),
ylab = "Nombre d'observations",
xlab = "Valeurs de theta estimées",
fill = ..count..
) + theme_minimal() + geom_vline(aes(xintercept = 1000), lty = 3) + scale_fill_gradient(low = "blue", high = "red") + annotate(
"text",
x = (min(estimations[[name]]) + 1000) / 2,
y = 5000 / 6,
label = paste("sd<theta> :", round(sd(
estimations[[name]]
), digits = 2)),
fontface = 2
) + annotate(
"text",
x = (min(estimations[[name]]) + 1000) / 2,
y = 5000 / 6  - 60,
label = paste("<theta> :", round(mean(
estimations[[name]]
), digits = 2)),
fontface = 2
)
)
moyenne_glissante <- vector()
for (i in 1:5000) {
moyenne_glissante[i] <- mean(estimations[[name]][1:i])
}
print(
qplot(
1:5000,
moyenne_glissante,
xlab = "Numéro de l'observation",
ylab = "Moyenne glissante"
) + geom_line() + geom_hline(yintercept = 1000, color = "red")
)
}
}
estimations <-
estimate_avec_remise(nombre_observations = 20, theta = 1000, m = 5000)
simul.confiance(100)
simul.confiance(100)
l <- simul.confiance(100)
simul.confiance(100)
intervalle <- function(observations, alpha) {
maximum <- max(observations)
n <- length(observations)
return(c(maximum, maximum / (alpha ^ (1 / n))))
}
frequency <- function(theta, n, alpha = 0.1) {
observations <- sample(1:theta, n, replace = T)
inter <- intervalle(observations, alpha)
return((theta >= inter[1] & theta <= inter[2]))
}
simul.confiance <- function(theta) {
nombre <- 10
echantillon.number <- round(seq(100, 5000, length.out = nombre))
echantillon.length <- round(seq(1, theta, length.out = nombre))
new_list <-
mapply(function(n, m) {
replicate(m, frequency(theta, n))
}, echantillon.length, echantillon.number)
percent <-
sapply(new_list, function(logi) {
sum(logi) / length(logi)
})
attr(percent, "echantillon.length") <- echantillon.length
attr(percent, "echantillon.number") <- echantillon.number
return(percent)
}
simul.confiance(100)
# theta = 1000 et n = 20
histogram(sample(1:1000, 20, replace = F))
estimations <-
estimate_sans_remise(nombre_observations = 20, theta = 1000, m = 5000)
# P(max <= k)
# k(k - 1)...(k - n + 1) / theta(theta - 1)...(theta - n + 1)
dunif.max <-
function(x, k, n) {
ifelse(x == k, 1, prod(k:(k - n + 1)) / prod(x:(x - n + 1)))
}
values_n5 <- vector(length = 299)
for (i in 100:400) {
values_n5[i - 99] <- dunif.max(x = i, k = 100, n = 5)
}
values_n10 <- vector(length = 299)
for (i in 100:400) {
values_n10[i - 99] <- dunif.max(x = i, k = 100, n = 10)
}
print(
ggplot(data.frame(y = values_n5, x = 100:400), aes(x, y)) + geom_point() +
theme_minimal() + ylab("Fonction de répartition du maximum des observations") + xlab("theta") + geom_hline(aes(yintercept = .05), lty = 3)
)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
histogram <- function(observations) {
# Histogramme des observations
par(mfcol = c(1, 2))
# classes de même largeur
# calcul du nombre de classes
k <- ceiling(log(length(observations)) / log(2)) + 1
a0 <-
min(observations) - 0.025 * (max(observations) - min(observations))
ak <-
max(observations) + 0.025 * (max(observations) - min(observations))
h <- (ak - a0) / k
breaks = seq(a0, ak, h)
histogram.largeur <- hist(
observations,
breaks = breaks,
prob = T,
main = "Histogramme même largeur",
ylab = "Nombre d'observations",
xlab = "Valeurs observées"
)
# classes de même effectif
nombre_elements <- length(observations) / k
observations.sorted <- sort(observations)
b <- nombre_elements * 1:k - 1
breaks <-
c(a0, (observations.sorted[b] + observations.sorted[b + 1]) / 2, ak)
histogram.effectif <- hist(
observations,
breaks = breaks,
prob = T,
main = "Histogramme même effectif",
ylab = "Nombre d'observations",
xlab = "Valeurs observées"
)
par(mfcol = c(1, 1))
}
probability_graph <-
function(nombre_observations, theta) {
observations <- sample(1:theta, nombre_observations, replace = T)
# nombre_lancers premières composantes du vecteur d'observations triées
observations.clip = sort(observations)[1:nombre_observations]
probabilites_cumulee = seq(1:nombre_observations) / theta
plot(
observations.clip,
probabilites_cumulee,
main = "Graphe de probabilités",
ylim = c(0, max(probabilites_cumulee)),
# doit rester inférieur à 1
xlim = c(1, theta),
col = "blue",
ylab = "Probabilités cumulées",
xlab = "Valeurs observées"
)
# Régression linéaire
observations.lm <-
lm(probabilites_cumulee ~ observations.clip)
print(coef(observations.lm))
print(summary(observations.lm)$r.squared)
# Tracé de la droite
clip(1, theta, 0, 1)
abline(observations.lm,
ylim = c(0, max(probabilites_cumulee)),
# doit rester inférieur à 1
xlim = c(1, theta))
# on récupère l'inverse de la pente de la droite de régression
theta_graph <- 1 / coef(observations.lm)[2]
return(theta_graph)
}
theta <- 1000
n <- 20
probability_graph(nombre_observations = n, theta = theta)
# ou plus simplement
plot(ecdf(sample(1:theta, n, replace = T)))
theta <- 1000
n <- 1000
probability_graph(nombre_observations = n, theta = theta)
theta <- 1000
n <- 1000
histogram(sample(1:theta, n, replace = T))
n <- 20
histogram(sample(1:theta, n, replace = T))
theta <- 1000
n <- 1000
hist(sample(1:theta, n, replace = T))
estimateur.graph <- function(theta, nombre_observations) {
observations = sample(1:theta, nombre_observations, replace = T)
observations.clip = sort(observations)[1:nombre_observations]
probabilites_cumulee = seq(1:nombre_observations) / theta
observations.lm <-
lm(probabilites_cumulee ~ observations.clip)
return(1 / coef(observations.lm)[2])
}
estimateur.moments <-
function(theta, nombre_observations, replace) {
observations = sample(1:theta, nombre_observations, replace = replace)
return(2 * mean(observations) - 1)
}
estimateur.mediane <-
function(theta, nombre_observations, replace) {
observations = sample(1:theta, nombre_observations, replace = replace)
return(2 * median(observations) - 1)
}
estimateur.max <- function(theta, nombre_observations, replace) {
observations = sample(1:theta, nombre_observations, replace = replace)
return(max(observations))
}
estimateur.variance_min <-
function(theta, nombre_observations, replace) {
observations = sample(1:theta, nombre_observations, replace = replace)
n = length(observations)
m = max(observations)
return((m ^ (n + 1) - (m - 1) ^ (n + 1)) / (m ^ n - (m - 1) ^ n))
}
estimate_avec_remise <- function(nombre_observations, theta, m = 5000) {
# Estimateur des moments
theta_moments <-
replicate(m,
estimateur.moments(theta, nombre_observations, replace = T))
# Estimateur de la médiane empirique
theta_mediane <-
replicate(m,
estimateur.mediane(theta, nombre_observations, replace = T))
# Estimateur du maximum de vraisemblance
theta_max <-
replicate(m, estimateur.max(theta, nombre_observations, replace = T))
# Estimateur graphique
theta_graph <-
replicate(m, estimateur.graph(theta, nombre_observations))
# Estimateur de variance minimale
theta_min <-
replicate(m,
estimateur.variance_min(theta, nombre_observations, replace = T))
return_list <-
list(
"moments" = theta_moments,
"mediane" = theta_mediane,
"max" = theta_max,
"graph" = theta_graph,
"variance_min" = theta_min
)
return(return_list)
}
bias <- function(observations, theta) {
return(mean(observations) - theta)
}
mse <- function(observations, theta) {
return(var(observations) + bias(observations, theta) ^ 2)
}
estimators.summary <- function(estimations, theta) {
# Si un estimateur est non biaisé, sa variance est égale à sa MSE
# La variance mesure la précision de l'estimateur.
# Le biais mesure son exactitude.
# Un estimateur avec une bonne MSE à une petite variance et un petit biais.
estimations.mean <- sapply(estimations, mean)
estimations.median <- sapply(estimations, median)
estimations.sd <- sapply(estimations, sd)
estimations.var <- sapply(estimations, var)
estimations.bias <- sapply(estimations, bias, theta = theta)
estimations.mse <- sapply(estimations, mse, theta = theta)
estimations.summary <- data.frame(
estimations.mean = estimations.mean,
estimations.median = estimations.median,
estimations.sd = estimations.sd,
estimations.var = estimations.var,
estimations.bias = estimations.bias,
estimations.mse = estimations.mse
)
print(t(round(estimations.summary, 2)))
}
estimations <-
estimate_avec_remise(nombre_observations = 20, theta = 1000, m = 1)
estimators.summary(estimations, theta = 1000)
estimations <-
estimate_avec_remise(nombre_observations = 20, theta = 1000, m = 5000)
estimators.summary(estimations, theta = 1000)
estimations <-
estimate_avec_remise(nombre_observations = 500, theta = 1000, m = 5000)
estimators.summary(estimations, theta = 1000)
estimations <-
estimate_avec_remise(nombre_observations = 1000, theta = 1000, m = 5000)
str(lapply(grouped, estimate))
lapply(grouped, estimate)
grouped;estimations <- lapply(grouped, estimate)
grouped.estimations <- lapply(grouped, estimate)
knitr::opts_chunk$set(echo = TRUE)
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
print(group.ks.test)
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
mean(lapply(grouped, estimate))
estimate <- function(iphones) {
n = length(iphones$NS)
m = max(iphones$NS)
return(c((1 + 1 / n) * m - 1, m + min(iphones$NS) - 1))
}
estimate(iphones.no.X)
# on effectue une petite translation de notre table pour l'affichage graphique
# 25 : 6 --> 25 : 0 pour 13 valeurs différentes
lookupWeek <- (rep(1:round(52 / 4), each = 4) + 6) %% 13
names(lookupWeek) <- sprintf("%02d", 1:52)
# récupération de la semaine
weekgroup <- function(NS) {
return(unname(lookupWeek[substr(NS, 4, 5)]))
}
iphones.no.X <-
cbind(iphones.no.X, weekgroup = weekgroup(iphones.no.X$PC))
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
mean(lapply(grouped, estimate))
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
lapply(grouped, estimate)
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
lapply(grouped, estimate)
grouped
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
lapply(lapply(grouped, estimate), mean)
estimate <- function(iphones) {
n = length(iphones$NS)
m = max(iphones$NS)
return(data.frame((1 + 1 / n) * m - 1, m + min(iphones$NS) - 1))
}
estimate(iphones.no.X)
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
lapply(grouped, estimate)
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
lapply(grouped, estimate)
colMeans(lapply(grouped, estimate))
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
Reduce(function(x, y) merge(x, y), lapply(grouped, estimate))
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
Reduce(function(x, y) merge(x, y), list(lapply(grouped, estimate)))
list(lapply(grouped, estimate))
lapply(grouped, estimate)
Reduce(function(x, y) merge(x, y), lapply(grouped, estimate))
Reduce(function(x, y) merge(x, y, all = T), lapply(grouped, estimate))
rowMeans(Reduce(function(x, y) merge(x, y, all = T), lapply(grouped, estimate)))
colMeans(Reduce(function(x, y) merge(x, y, all = T), lapply(grouped, estimate)))
grouped.summary <- lapply(grouped, iphones.summary)
group.ks.test <- lapply(grouped, ks)
# group.chisq.test <- lapply(grouped, chisq)
mean(colMeans(Reduce(function(x, y) merge(x, y, all = T), lapply(grouped, estimate))))
grouped.mean <- colMeans(Reduce(function(x, y) merge(x, y, all = T), lapply(grouped, estimate)))
mean(grouped.mean)
grouped.mean <- colMeans(Reduce(function(x, y) merge(x, y, all = T), lapply(grouped, estimate)))
print(grouped.mean)
mean(grouped.mean)
